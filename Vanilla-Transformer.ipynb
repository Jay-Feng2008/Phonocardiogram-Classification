{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58befd64",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c7075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, \"vanilla-transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c251f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embd import PositionalEmbedding\n",
    "from encoder import EncoderLayer_postLN, EncoderLayer_preLN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c1d6cd",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7729518",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "def model():\n",
    "    inputs = keras.layers.Input((127, 15), batch_size=BATCH_SIZE)\n",
    "    x = PositionalEmbedding(32)(inputs)\n",
    "    x = EncoderLayer_postLN(d_model=32, num_heads=128, dff=64)(x)\n",
    "    x = EncoderLayer_postLN(d_model=32, num_heads=64, dff=32)(x)\n",
    "    x = keras.layers.GlobalAveragePooling1D(data_format='channels_first')(x)\n",
    "    x = keras.layers.Dense(5, activation='softmax')(x)\n",
    "    return keras.Model(inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7834d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35afbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal((BATCH_SIZE, 127, 15))\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab9046",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67063714",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"mfcc.npz\")\n",
    "X = data[\"X\"]\n",
    "X_mask = data[\"X_mask\"]\n",
    "Y = data[\"Y\"]\n",
    "\n",
    "x_train = X[0:832]\n",
    "x_mask_train = X_mask[0:832]\n",
    "y_train = Y[0:832]\n",
    "x_test = X[872:]\n",
    "y_test = Y[872:]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train, x_mask_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=10000).batch(BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_dataset = val_dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE)\n",
    "\n",
    "x = x_train[0:BATCH_SIZE]\n",
    "x_rank = tf.rank(x).numpy()\n",
    "x_norm_resize_shape = [BATCH_SIZE] + list(tf.ones(tf.rank(x), dtype=tf.int32).numpy())[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c45212",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b52fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69462829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturbation_loss(x, y, from_logits=False):\n",
    "    return keras.losses.CategoricalCrossentropy(from_logits=from_logits)(x, y)\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "learning_rate = CustomSchedule(32)\n",
    "optimizer = keras.optimizers.experimental.AdamW(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_loss_metric = keras.losses.SparseCategoricalCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bc7bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 8.     # the perturbation parameter\n",
    "sig = 1e-5   # initial perturbation StdDev\n",
    "zeta = 1e-6  # differentiation constant\n",
    "lamd = 1     # regularization parameter\n",
    "\n",
    "@tf.function\n",
    "def training_step(x, label, x_mask):\n",
    "    x_p = tf.random.normal(x.shape, stddev=sig)\n",
    "    x_norm = x_p\n",
    "    for i in range(x_rank-1, 0, -1):\n",
    "        x_norm = tf.norm(x_norm, ord=2, axis=int(i))\n",
    "    x_p /= tf.reshape(x_norm, (BATCH_SIZE, 1, 1))\n",
    "    x_p *= zeta\n",
    "\n",
    "\n",
    "    with tf.GradientTape() as adversarial_tape:\n",
    "        adversarial_tape.watch(x_p)\n",
    "        y_p = model(x + x_p, training=True)\n",
    "        y = model(x, training=True)\n",
    "        l = perturbation_loss(y, y_p)\n",
    "    g = adversarial_tape.gradient(l, x_p)\n",
    "    print(g[0])\n",
    "\n",
    "    g_norm = g\n",
    "    for i in range(x_rank-1, 0, -1):\n",
    "        g_norm = tf.norm(g_norm, ord=2, axis=int(i))\n",
    "\n",
    "    x_p = eps * g / tf.reshape(g_norm, x_norm_resize_shape)\n",
    "    x_p *= x_mask\n",
    "\n",
    "    with tf.GradientTape() as model_tape:\n",
    "        y_p = model(x + x_p, training=True)\n",
    "        y = model(x, training=True)\n",
    "        l = perturbation_loss(y, y_p)    # Recalculate regularization\n",
    "\n",
    "        logits = model(x, training=True)\n",
    "        loss = loss_fn(label, logits) + lamd * l / BATCH_SIZE\n",
    "    grads = model_tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    train_acc_metric.update_state(label, logits)\n",
    "    return loss, train_acc_metric.result(), l, x_p, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43f6a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_metric = []\n",
    "val_metric = []\n",
    "val_loss = []\n",
    "p_loss = []\n",
    "for epoch in tqdm(range(1000)):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    for step, (x, label, x_mask) in enumerate(train_dataset):\n",
    "        loss, train_acc, l, x_p, g = training_step(x, label, x_mask)\n",
    "\n",
    "    print(\n",
    "        \"Training loss: %.4f\\nTraining metric: %.4f\"\n",
    "        % (float(loss), float(train_acc))\n",
    "    )\n",
    "    print(\"perturbation loss: %.4f\" % float(l))\n",
    "\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        # Update val metrics\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "        v_loss = val_loss_metric(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc)))\n",
    "\n",
    "    train_loss.append(loss)\n",
    "    train_metric.append(train_acc)\n",
    "    val_metric.append(val_acc)\n",
    "    val_loss.append(v_loss)\n",
    "    p_loss.append(l)\n",
    "\n",
    "    tl = np.array(train_loss)\n",
    "    tm = np.array(train_metric)\n",
    "    vm = np.array(val_metric)\n",
    "    vl = np.array(val_loss)\n",
    "    pl = np.array(p_loss)\n",
    "\n",
    "    np.savez(\"logs.npz\", train_loss=tl, train_acc=tm, val_acc=vm, p_loss=pl, val_loss=vl, x_p=x_p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
