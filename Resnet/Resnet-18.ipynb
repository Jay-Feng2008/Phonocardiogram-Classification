{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6cSmwmm5EKqd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7IQfYMNzEgk5"
      },
      "outputs": [],
      "source": [
        "def resblock(inputs, num_feature_maps):\n",
        "    # BLOCK 1\n",
        "\n",
        "    conv_x = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=3, padding='same')(inputs)\n",
        "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "    conv_y = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=3, padding='same')(conv_x)\n",
        "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "    conv_z = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
        "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "\n",
        "    # expand channels for the sum\n",
        "    shortcut_y = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=1, padding='same')(inputs)\n",
        "    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "    outputs = keras.layers.add([shortcut_y, conv_y])\n",
        "    outputs = keras.layers.Activation('relu')(outputs)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "blyLWsByHhff"
      },
      "outputs": [],
      "source": [
        "def build_model(input_shape, batch_size):\n",
        "    inputs = keras.layers.Input(shape=input_shape, batch_size=32)\n",
        "\n",
        "    layer1 = resblock(inputs, 64)\n",
        "    layer1 = resblock(layer1, 64)\n",
        "\n",
        "    layer2 = resblock(layer1, 128)\n",
        "    layer2 = resblock(layer2, 128)\n",
        "\n",
        "    layer3 = resblock(layer1, 256)\n",
        "    layer3 = resblock(layer2, 256)\n",
        "\n",
        "    layer3 = resblock(layer1, 512)\n",
        "    layer3 = resblock(layer2, 512)\n",
        "\n",
        "    pool = keras.layers.GlobalAveragePooling1D(data_format='channels_first')(layer3)\n",
        "\n",
        "    output = keras.layers.Dense(5, activation='softmax')(pool)\n",
        "\n",
        "\n",
        "\n",
        "    return keras.models.Model(inputs=inputs, outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "godvi6cdJOBT"
      },
      "outputs": [],
      "source": [
        "model = build_model((137, 15), 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GKtU5wDzJT5l"
      },
      "outputs": [],
      "source": [
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2As3kQpYLdP2"
      },
      "outputs": [],
      "source": [
        "f = np.load(\"mfcc.npz\")\n",
        "X, Y = f['X'], f['Y']\n",
        "x_train =  X[100:1000]\n",
        "y_train =  Y[100:1000]\n",
        "\n",
        "\n",
        "x_test, y_test = X[0:100], Y[0:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nyri0zLjtsQA"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train[0:800], y_train[0:800]))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=800, reshuffle_each_iteration=True).batch(32, drop_remainder=True)\n",
        "x_val = x_train[800:]\n",
        "y_val = y_train[800:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YwrYtJKOL-5_"
      },
      "outputs": [],
      "source": [
        "# optimizer = keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, weight_decay=0.0001)\n",
        "# model.compile(optimizer=optimizer, loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "k2mGwgRIMX7s"
      },
      "outputs": [],
      "source": [
        "# model.fit(x_train[], y_train, batch_size=32, epochs=2000, validation_data=(x_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iew0g_9ftsQA",
        "outputId": "97eb78cd-48b1-46e7-ec28-1b9b1ca14080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [26:03<00:00,  1.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def training_step(x, y):\n",
        "\n",
        "    with tf.GradientTape() as model_tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss = loss_fn(y, logits)\n",
        "    grads = model_tape.gradient(loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    acc_metric.update_state(y, logits)\n",
        "    acc = acc_metric.result()\n",
        "    acc_metric.reset_states()\n",
        "\n",
        "    return loss, acc\n",
        "\n",
        "log = {\"training_loss\":[], \"training_acc\":[],\n",
        "        \"val_loss\":[], \"val_acc\":[], \"test_acc\":[], \"test_logits\":[]}\n",
        "log_path = \"log\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \".npy\"\n",
        "for epoch in tqdm(range(2000)):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for step, (x, y) in enumerate(train_dataset):\n",
        "        batch_loss, batch_acc = training_step(x, y)\n",
        "        epoch_loss += float(batch_loss)\n",
        "        epoch_acc += float(batch_acc)\n",
        "    epoch_loss /= step+1\n",
        "    epoch_acc /= step+1\n",
        "\n",
        "    val_loss = 0\n",
        "    val_acc = 0\n",
        "    val_logits = model(x_val, training=False)\n",
        "    val_loss = loss_fn(y_val, val_logits)\n",
        "    acc_metric.update_state(y_val, val_logits)\n",
        "    val_acc = acc_metric.result().numpy()\n",
        "    acc_metric.reset_states()\n",
        "\n",
        "    # print(\"Validation loss: %.4f\" % (float(val_loss)))\n",
        "    # print(\"Validation acc: %.4f\" % (float(val_acc)))\n",
        "\n",
        "    test_logits = model(x_test, training=False)\n",
        "    acc_metric.update_state(y_test, test_logits)\n",
        "    test_acc = acc_metric.result().numpy()\n",
        "    acc_metric.reset_states()\n",
        "\n",
        "    log[\"training_loss\"].append(epoch_loss)\n",
        "    log[\"training_acc\"].append(epoch_acc)\n",
        "    log[\"val_loss\"].append(val_loss)\n",
        "    log[\"val_acc\"].append(val_acc)\n",
        "    log[\"test_acc\"].append(test_acc)\n",
        "    log[\"test_logits\"].append(test_logits)\n",
        "\n",
        "\n",
        "    np.save(log_path, [log])\n",
        "\n",
        "log['test_acc'] = np.array(log['test_acc'])\n",
        "log['val_loss'] = np.array(log['val_loss'])\n",
        "testing_metric = 0\n",
        "if len(log['test_acc'][np.where(log['val_loss']-min(log['val_loss'])<1e-6)]) != 0:\n",
        "    testing_metric = log['test_acc'][np.where((max(log['val_acc']-log['val_loss']) - (log['val_acc']-log['val_loss']))<1e-6)][0]\n",
        "print(testing_metric)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eU01dF0SCk_3"
      },
      "execution_count": 10,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}