{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6cSmwmm5EKqd"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.load(\"mfcc_fixed.npz\")\n",
    "X, Y = f['X'], f['Y']\n",
    "X_train =  X[0:900]\n",
    "Y_train =  Y[0:900]        # fold 10\n",
    "\n",
    "\n",
    "x_test, y_test = X[900:1000], Y[900:1000]\n",
    "x_train, y_train = X_train[0:800], Y_train[0:800]\n",
    "x_val, y_val= X_train[800:900], Y_train[800:900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=800, reshuffle_each_iteration=True).batch(batch_size, drop_remainder=True)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.shuffle(buffer_size=800, reshuffle_each_iteration=True).batch(batch_size, drop_remainder=True)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.shuffle(buffer_size=800, reshuffle_each_iteration=True).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "\n",
    "x = x_train[0:batch_size]\n",
    "x_rank = tf.rank(x).numpy()\n",
    "x_norm_resize_shape = [batch_size] + list(tf.ones(tf.rank(x), dtype=tf.int32).numpy())[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "7IQfYMNzEgk5"
   },
   "outputs": [],
   "source": [
    "def resblock(inputs, num_feature_maps):\n",
    "    # BLOCK 1\n",
    "\n",
    "    conv_x = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=3, padding='same')(inputs)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=3, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "\n",
    "    # expand channels for the sum\n",
    "    shortcut_y = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=1, padding='same')(inputs)\n",
    "    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "    outputs = keras.layers.add([shortcut_y, conv_y])\n",
    "    outputs = keras.layers.Activation('relu')(outputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "blyLWsByHhff"
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape, batch_size):\n",
    "    inputs = keras.layers.Input(shape=input_shape, batch_size=32)\n",
    "\n",
    "    layer1 = resblock(inputs, 64)\n",
    "    layer1 = resblock(layer1, 64)\n",
    "\n",
    "    layer2 = resblock(layer1, 128)\n",
    "    layer2 = resblock(layer2, 128)\n",
    "\n",
    "    layer3 = resblock(layer1, 256)\n",
    "    layer3 = resblock(layer2, 256)\n",
    "\n",
    "    layer3 = resblock(layer1, 512)\n",
    "    layer3 = resblock(layer2, 512)\n",
    "\n",
    "    pool = keras.layers.GlobalAveragePooling1D(data_format='channels_first')(layer3)\n",
    "\n",
    "    output = keras.layers.Dense(5, activation='softmax')(pool)\n",
    "\n",
    "\n",
    "\n",
    "    return keras.models.Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "godvi6cdJOBT"
   },
   "outputs": [],
   "source": [
    "model = build_model(X[0].shape, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "GKtU5wDzJT5l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)       [(32, 54, 30)]               0         []                            \n",
      "                                                                                                  \n",
      " conv1d_320 (Conv1D)         (32, 54, 64)                 5824      ['input_11[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_320 (B  (32, 54, 64)                 256       ['conv1d_320[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " activation_240 (Activation  (32, 54, 64)                 0         ['batch_normalization_320[0][0\n",
      " )                                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_321 (Conv1D)         (32, 54, 64)                 12352     ['activation_240[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_323 (Conv1D)         (32, 54, 64)                 1984      ['input_11[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_321 (B  (32, 54, 64)                 256       ['conv1d_321[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_323 (B  (32, 54, 64)                 256       ['conv1d_323[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " activation_241 (Activation  (32, 54, 64)                 0         ['batch_normalization_321[0][0\n",
      " )                                                                  ]']                           \n",
      "                                                                                                  \n",
      " add_80 (Add)                (32, 54, 64)                 0         ['batch_normalization_323[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'activation_241[0][0]']      \n",
      "                                                                                                  \n",
      " activation_242 (Activation  (32, 54, 64)                 0         ['add_80[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv1d_324 (Conv1D)         (32, 54, 64)                 12352     ['activation_242[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_324 (B  (32, 54, 64)                 256       ['conv1d_324[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " activation_243 (Activation  (32, 54, 64)                 0         ['batch_normalization_324[0][0\n",
      " )                                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_325 (Conv1D)         (32, 54, 64)                 12352     ['activation_243[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_327 (Conv1D)         (32, 54, 64)                 4160      ['activation_242[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_325 (B  (32, 54, 64)                 256       ['conv1d_325[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_327 (B  (32, 54, 64)                 256       ['conv1d_327[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " activation_244 (Activation  (32, 54, 64)                 0         ['batch_normalization_325[0][0\n",
      " )                                                                  ]']                           \n",
      "                                                                                                  \n",
      " add_81 (Add)                (32, 54, 64)                 0         ['batch_normalization_327[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'activation_244[0][0]']      \n",
      "                                                                                                  \n",
      " activation_245 (Activation  (32, 54, 64)                 0         ['add_81[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv1d_328 (Conv1D)         (32, 54, 128)                24704     ['activation_245[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_328 (B  (32, 54, 128)                512       ['conv1d_328[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " activation_246 (Activation  (32, 54, 128)                0         ['batch_normalization_328[0][0\n",
      " )                                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_329 (Conv1D)         (32, 54, 128)                49280     ['activation_246[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_331 (Conv1D)         (32, 54, 128)                8320      ['activation_245[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_329 (B  (32, 54, 128)                512       ['conv1d_329[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_331 (B  (32, 54, 128)                512       ['conv1d_331[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " activation_247 (Activation  (32, 54, 128)                0         ['batch_normalization_329[0][0\n",
      " )                                                                  ]']                           \n",
      "                                                                                                  \n",
      " add_82 (Add)                (32, 54, 128)                0         ['batch_normalization_331[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'activation_247[0][0]']      \n",
      "                                                                                                  \n",
      " activation_248 (Activation  (32, 54, 128)                0         ['add_82[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv1d_332 (Conv1D)         (32, 54, 128)                49280     ['activation_248[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_332 (B  (32, 54, 128)                512       ['conv1d_332[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " activation_249 (Activation  (32, 54, 128)                0         ['batch_normalization_332[0][0\n",
      " )                                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_333 (Conv1D)         (32, 54, 128)                49280     ['activation_249[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_335 (Conv1D)         (32, 54, 128)                16512     ['activation_248[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_333 (B  (32, 54, 128)                512       ['conv1d_333[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_335 (B  (32, 54, 128)                512       ['conv1d_335[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " activation_250 (Activation  (32, 54, 128)                0         ['batch_normalization_333[0][0\n",
      " )                                                                  ]']                           \n",
      "                                                                                                  \n",
      " add_83 (Add)                (32, 54, 128)                0         ['batch_normalization_335[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'activation_250[0][0]']      \n",
      "                                                                                                  \n",
      " activation_251 (Activation  (32, 54, 128)                0         ['add_83[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv1d_348 (Conv1D)         (32, 54, 512)                197120    ['activation_251[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_348 (B  (32, 54, 512)                2048      ['conv1d_348[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " activation_261 (Activation  (32, 54, 512)                0         ['batch_normalization_348[0][0\n",
      " )                                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_349 (Conv1D)         (32, 54, 512)                786944    ['activation_261[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_351 (Conv1D)         (32, 54, 512)                66048     ['activation_251[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_349 (B  (32, 54, 512)                2048      ['conv1d_349[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_351 (B  (32, 54, 512)                2048      ['conv1d_351[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " activation_262 (Activation  (32, 54, 512)                0         ['batch_normalization_349[0][0\n",
      " )                                                                  ]']                           \n",
      "                                                                                                  \n",
      " add_87 (Add)                (32, 54, 512)                0         ['batch_normalization_351[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'activation_262[0][0]']      \n",
      "                                                                                                  \n",
      " activation_263 (Activation  (32, 54, 512)                0         ['add_87[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (32, 54)                     0         ['activation_263[0][0]']      \n",
      " 0 (GlobalAveragePooling1D)                                                                       \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (32, 5)                      275       ['global_average_pooling1d_10[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1307539 (4.99 MB)\n",
      "Trainable params: 1302163 (4.97 MB)\n",
      "Non-trainable params: 5376 (21.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "YwrYtJKOL-5_"
   },
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "lds = lambda x, y: tf.math.reduce_sum(keras.losses.kl_divergence(x, y))\n",
    "acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "rLRoP_callback = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                                    factor=0.1,\n",
    "                                                    patience=10,\n",
    "                                                    verbose=0,\n",
    "                                                    mode=\"auto\",\n",
    "                                                    min_delta=0.0001,\n",
    "                                                    cooldown=0,\n",
    "                                                    min_lr=0.0)\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "                                            monitor=\"val_loss\",\n",
    "                                            min_delta=0,\n",
    "                                            patience=100,\n",
    "                                            verbose=1,\n",
    "                                            mode=\"auto\",\n",
    "                                            baseline=None,\n",
    "                                            restore_best_weights=True,\n",
    "                                            start_from_epoch=0)\n",
    "                                                    \n",
    "model.compile(optimizer=optimizer, loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "k2mGwgRIMX7s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "8/8 [==============================] - 7s 78ms/step - loss: 1.7306 - accuracy: 0.3925 - val_loss: 1.7236 - val_accuracy: 0.2800 - lr: 0.1000\n",
      "Epoch 2/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.7126 - accuracy: 0.7462 - val_loss: 5.1480 - val_accuracy: 0.2000 - lr: 0.1000\n",
      "Epoch 3/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.3154 - accuracy: 0.8925 - val_loss: 6.6804 - val_accuracy: 0.4300 - lr: 0.1000\n",
      "Epoch 4/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1642 - accuracy: 0.9413 - val_loss: 11.9481 - val_accuracy: 0.3800 - lr: 0.1000\n",
      "Epoch 5/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1112 - accuracy: 0.9638 - val_loss: 11.3483 - val_accuracy: 0.3700 - lr: 0.1000\n",
      "Epoch 6/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0786 - accuracy: 0.9775 - val_loss: 7.7935 - val_accuracy: 0.3800 - lr: 0.1000\n",
      "Epoch 7/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0363 - accuracy: 0.9937 - val_loss: 9.8988 - val_accuracy: 0.3800 - lr: 0.1000\n",
      "Epoch 8/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0205 - accuracy: 0.9975 - val_loss: 10.2471 - val_accuracy: 0.3900 - lr: 0.1000\n",
      "Epoch 9/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0106 - accuracy: 0.9987 - val_loss: 6.9908 - val_accuracy: 0.4000 - lr: 0.1000\n",
      "Epoch 10/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 6.8406 - val_accuracy: 0.4100 - lr: 0.1000\n",
      "Epoch 11/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0102 - accuracy: 0.9987 - val_loss: 7.0554 - val_accuracy: 0.4100 - lr: 0.1000\n",
      "Epoch 12/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0336 - accuracy: 0.9862 - val_loss: 6.8920 - val_accuracy: 0.4300 - lr: 0.0100\n",
      "Epoch 13/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 6.8578 - val_accuracy: 0.5000 - lr: 0.0100\n",
      "Epoch 14/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 6.0839 - val_accuracy: 0.5000 - lr: 0.0100\n",
      "Epoch 15/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 5.2133 - val_accuracy: 0.5200 - lr: 0.0100\n",
      "Epoch 16/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 4.3818 - val_accuracy: 0.5200 - lr: 0.0100\n",
      "Epoch 17/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.5933 - val_accuracy: 0.5300 - lr: 0.0100\n",
      "Epoch 18/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.8922 - val_accuracy: 0.5400 - lr: 0.0100\n",
      "Epoch 19/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.2853 - val_accuracy: 0.5700 - lr: 0.0100\n",
      "Epoch 20/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.7803 - val_accuracy: 0.6500 - lr: 0.0100\n",
      "Epoch 21/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.3699 - val_accuracy: 0.7200 - lr: 0.0100\n",
      "Epoch 22/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.0728 - val_accuracy: 0.7800 - lr: 0.0100\n",
      "Epoch 23/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8403 - val_accuracy: 0.7900 - lr: 0.0100\n",
      "Epoch 24/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6560 - val_accuracy: 0.8500 - lr: 0.0100\n",
      "Epoch 25/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.5157 - val_accuracy: 0.8800 - lr: 0.0100\n",
      "Epoch 26/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4140 - val_accuracy: 0.9200 - lr: 0.0100\n",
      "Epoch 27/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3401 - val_accuracy: 0.9200 - lr: 0.0100\n",
      "Epoch 28/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2815 - val_accuracy: 0.9300 - lr: 0.0100\n",
      "Epoch 29/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2347 - val_accuracy: 0.9300 - lr: 0.0100\n",
      "Epoch 30/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2005 - val_accuracy: 0.9500 - lr: 0.0100\n",
      "Epoch 31/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1748 - val_accuracy: 0.9600 - lr: 0.0100\n",
      "Epoch 32/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1574 - val_accuracy: 0.9700 - lr: 0.0100\n",
      "Epoch 33/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1457 - val_accuracy: 0.9700 - lr: 0.0100\n",
      "Epoch 34/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1374 - val_accuracy: 0.9700 - lr: 0.0100\n",
      "Epoch 35/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1306 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 36/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1249 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 37/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1210 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 38/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1176 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 39/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 9.6321e-04 - accuracy: 1.0000 - val_loss: 0.1148 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 40/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 9.7180e-04 - accuracy: 1.0000 - val_loss: 0.1124 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 41/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 9.9559e-04 - accuracy: 1.0000 - val_loss: 0.1101 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 42/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1085 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 43/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 9.8137e-04 - accuracy: 1.0000 - val_loss: 0.1070 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 44/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.1058 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 45/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 9.4158e-04 - accuracy: 1.0000 - val_loss: 0.1048 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 46/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 8.3248e-04 - accuracy: 1.0000 - val_loss: 0.1037 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 47/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 9.1502e-04 - accuracy: 1.0000 - val_loss: 0.1029 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 48/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 9.3971e-04 - accuracy: 1.0000 - val_loss: 0.1021 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 49/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 9.5466e-04 - accuracy: 1.0000 - val_loss: 0.1014 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 50/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 8.9798e-04 - accuracy: 1.0000 - val_loss: 0.1006 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 51/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 8.9466e-04 - accuracy: 1.0000 - val_loss: 0.0999 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 52/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 8.9208e-04 - accuracy: 1.0000 - val_loss: 0.0994 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 53/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 8.3246e-04 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 54/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 7.7601e-04 - accuracy: 1.0000 - val_loss: 0.0982 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 55/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 7.6834e-04 - accuracy: 1.0000 - val_loss: 0.0978 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 56/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 7.7902e-04 - accuracy: 1.0000 - val_loss: 0.0973 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 57/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 9.0593e-04 - accuracy: 1.0000 - val_loss: 0.0970 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 58/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 7.8257e-04 - accuracy: 1.0000 - val_loss: 0.0968 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 59/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 7.4451e-04 - accuracy: 1.0000 - val_loss: 0.0965 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 60/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 7.8590e-04 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 61/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 7.7715e-04 - accuracy: 1.0000 - val_loss: 0.0957 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 62/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 7.6244e-04 - accuracy: 1.0000 - val_loss: 0.0958 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 63/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.5613e-04 - accuracy: 1.0000 - val_loss: 0.0958 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 64/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.5655e-04 - accuracy: 1.0000 - val_loss: 0.0958 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 65/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 7.2088e-04 - accuracy: 1.0000 - val_loss: 0.0959 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 66/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 7.1134e-04 - accuracy: 1.0000 - val_loss: 0.0959 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 67/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.6736e-04 - accuracy: 1.0000 - val_loss: 0.0958 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 68/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 7.5891e-04 - accuracy: 1.0000 - val_loss: 0.0959 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 69/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.0905e-04 - accuracy: 1.0000 - val_loss: 0.0958 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 70/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 6.3149e-04 - accuracy: 1.0000 - val_loss: 0.0956 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 71/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.5921e-04 - accuracy: 1.0000 - val_loss: 0.0957 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 72/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 7.8071e-04 - accuracy: 1.0000 - val_loss: 0.0957 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 73/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.4176e-04 - accuracy: 1.0000 - val_loss: 0.0957 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 74/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.8400e-04 - accuracy: 1.0000 - val_loss: 0.0958 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 75/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.6283e-04 - accuracy: 1.0000 - val_loss: 0.0958 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 76/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 5.9668e-04 - accuracy: 1.0000 - val_loss: 0.0958 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 77/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.0309e-04 - accuracy: 1.0000 - val_loss: 0.0958 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 78/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.4951e-04 - accuracy: 1.0000 - val_loss: 0.0959 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 79/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 7.2895e-04 - accuracy: 1.0000 - val_loss: 0.0959 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 80/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.1718e-04 - accuracy: 1.0000 - val_loss: 0.0959 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 81/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.3439e-04 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 82/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.0896e-04 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 83/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.0874e-04 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 84/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.6159e-04 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 85/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.3322e-04 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 86/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.4193e-04 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 87/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.3179e-04 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 88/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.2240e-04 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 89/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 5.9497e-04 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 90/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.8996e-04 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 91/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.1750e-04 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 92/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.0968e-04 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9800 - lr: 1.0000e-05\n",
      "Epoch 93/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.3390e-04 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9800 - lr: 1.0000e-05\n",
      "Epoch 94/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.2807e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-05\n",
      "Epoch 95/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 6.0224e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-05\n",
      "Epoch 96/2000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 6.2590e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-05\n",
      "Epoch 97/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 7.1966e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-05\n",
      "Epoch 98/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 6.3754e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-05\n",
      "Epoch 99/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.0338e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-05\n",
      "Epoch 100/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.6248e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-05\n",
      "Epoch 101/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.9056e-04 - accuracy: 1.0000 - val_loss: 0.0963 - val_accuracy: 0.9800 - lr: 1.0000e-05\n",
      "Epoch 102/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.3973e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-06\n",
      "Epoch 103/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 6.7840e-04 - accuracy: 1.0000 - val_loss: 0.0963 - val_accuracy: 0.9800 - lr: 1.0000e-06\n",
      "Epoch 104/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 5.9152e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-06\n",
      "Epoch 105/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.3232e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-06\n",
      "Epoch 106/2000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 7.1117e-04 - accuracy: 1.0000 - val_loss: 0.0963 - val_accuracy: 0.9800 - lr: 1.0000e-06\n",
      "Epoch 107/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 6.2921e-04 - accuracy: 1.0000 - val_loss: 0.0963 - val_accuracy: 0.9800 - lr: 1.0000e-06\n",
      "Epoch 108/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 5.9658e-04 - accuracy: 1.0000 - val_loss: 0.0963 - val_accuracy: 0.9800 - lr: 1.0000e-06\n",
      "Epoch 109/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.0215e-04 - accuracy: 1.0000 - val_loss: 0.0963 - val_accuracy: 0.9800 - lr: 1.0000e-06\n",
      "Epoch 110/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.2036e-04 - accuracy: 1.0000 - val_loss: 0.0963 - val_accuracy: 0.9800 - lr: 1.0000e-06\n",
      "Epoch 111/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 5.9932e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-06\n",
      "Epoch 112/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.2627e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-07\n",
      "Epoch 113/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 5.9798e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-07\n",
      "Epoch 114/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 6.6268e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-07\n",
      "Epoch 115/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.7832e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-07\n",
      "Epoch 116/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.6682e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-07\n",
      "Epoch 117/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.4743e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-07\n",
      "Epoch 118/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 5.7865e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-07\n",
      "Epoch 119/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 5.7596e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-07\n",
      "Epoch 120/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.3742e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-07\n",
      "Epoch 121/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 5.9770e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-07\n",
      "Epoch 122/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.6013e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-08\n",
      "Epoch 123/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 5.8389e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-08\n",
      "Epoch 124/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 6.2455e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-08\n",
      "Epoch 125/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 5.7489e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-08\n",
      "Epoch 126/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 7.0286e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-08\n",
      "Epoch 127/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 6.1220e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-08\n",
      "Epoch 128/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.6669e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-08\n",
      "Epoch 129/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.4703e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-08\n",
      "Epoch 130/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.3211e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-08\n",
      "Epoch 131/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.7310e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-08\n",
      "Epoch 132/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 5.9271e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-09\n",
      "Epoch 133/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.1959e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-09\n",
      "Epoch 134/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 6.8929e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-09\n",
      "Epoch 135/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 6.8774e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-09\n",
      "Epoch 136/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.4481e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-09\n",
      "Epoch 137/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 6.9848e-04 - accuracy: 1.0000 - val_loss: 0.0963 - val_accuracy: 0.9800 - lr: 1.0000e-09\n",
      "Epoch 138/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 5.7641e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-09\n",
      "Epoch 139/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.1375e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-09\n",
      "Epoch 140/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 6.2229e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-09\n",
      "Epoch 141/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.4044e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-09\n",
      "Epoch 142/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.6368e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-10\n",
      "Epoch 143/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 6.5297e-04 - accuracy: 1.0000 - val_loss: 0.0963 - val_accuracy: 0.9800 - lr: 1.0000e-10\n",
      "Epoch 144/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.3202e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-10\n",
      "Epoch 145/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.0789e-04 - accuracy: 1.0000 - val_loss: 0.0963 - val_accuracy: 0.9800 - lr: 1.0000e-10\n",
      "Epoch 146/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.4049e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-10\n",
      "Epoch 147/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.8281e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-10\n",
      "Epoch 148/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.2929e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-10\n",
      "Epoch 149/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.9350e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-10\n",
      "Epoch 150/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 5.8162e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-10\n",
      "Epoch 151/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.5561e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-10\n",
      "Epoch 152/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 5.6496e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-11\n",
      "Epoch 153/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.4873e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-11\n",
      "Epoch 154/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.4181e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-11\n",
      "Epoch 155/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.4125e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-11\n",
      "Epoch 156/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.9435e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-11\n",
      "Epoch 157/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.3787e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-11\n",
      "Epoch 158/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 7.9022e-04 - accuracy: 1.0000 - val_loss: 0.0963 - val_accuracy: 0.9800 - lr: 1.0000e-11\n",
      "Epoch 159/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.0054e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-11\n",
      "Epoch 160/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.1179e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-11\n",
      "Epoch 161/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 5.7083e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-11\n",
      "Epoch 162/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.6716e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-12\n",
      "Epoch 163/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.5008e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-12\n",
      "Epoch 164/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.2768e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-12\n",
      "Epoch 165/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 6.9325e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-12\n",
      "Epoch 166/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 6.5720e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-12\n",
      "Epoch 167/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 6.8988e-04 - accuracy: 1.0000 - val_loss: 0.0963 - val_accuracy: 0.9800 - lr: 1.0000e-12\n",
      "Epoch 168/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 5.8345e-04 - accuracy: 1.0000 - val_loss: 0.0963 - val_accuracy: 0.9800 - lr: 1.0000e-12\n",
      "Epoch 169/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 5.8607e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-12\n",
      "Epoch 170/2000\n",
      "5/8 [=================>............] - ETA: 0s - loss: 4.9526e-04 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 70.\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 6.1919e-04 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9800 - lr: 1.0000e-12\n",
      "Epoch 170: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f6ccb6e8250>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, validation_data=val_dataset, batch_size=batch_size, epochs=2000,\n",
    "          callbacks=[es_callback, rLRoP_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "eU01dF0SCk_3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.010352697\n"
     ]
    }
   ],
   "source": [
    "acc_metric.reset_states()\n",
    "acc_metric.update_state(y_test, model(x_test))\n",
    "acc = acc_metric.result().numpy()\n",
    "loss = loss_fn(y_test, model(x_test)).numpy()\n",
    "print(acc, loss)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
